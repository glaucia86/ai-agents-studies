# Biblioteca de Agente Dummy (Do Zero)

## Planejamento da Unidade 1

Este curso √© **framework-agn√≥stico** porque queremos focar nos conceitos de agentes de IA e evitar nos emaranhamos nas especificidades de um framework particular.

Al√©m disso, queremos que os estudantes sejam capazes de usar os conceitos que aprendem neste curso em seus pr√≥prios projetos, usando qualquer framework que gostem.

Portanto, para esta Unidade 1, usaremos uma **biblioteca de agente dummy** e uma API serverless simples para acessar nosso motor LLM.

Voc√™ provavelmente n√£o usaria essas em produ√ß√£o, mas elas servir√£o como um bom ponto de partida para entender como os agentes funcionam.

Ap√≥s esta se√ß√£o, voc√™ estar√° pronto para criar um Agente simples usando smolagents.

E nas Unidades seguintes tamb√©m usaremos outras bibliotecas de Agentes de IA como LangGraph e LlamaIndex.

Para manter as coisas simples, usaremos fun√ß√µes TypeScript simples como Tools e Agent.

Usaremos pacotes built-in do Node.js e algumas bibliotecas comuns para que voc√™ possa experimentar em qualquer ambiente.

## API Serverless

No ecossistema Hugging Face, h√° um recurso conveniente chamado **API Serverless** que permite executar facilmente infer√™ncia em muitos modelos. N√£o h√° instala√ß√£o ou implanta√ß√£o necess√°ria.

### Configura√ß√£o Inicial em TypeScript

```typescript
// Instala√ß√£o necess√°ria:
// npm install @huggingface/inference axios dotenv
// npm install -D @types/node

import { HfInference } from '@huggingface/inference';
import * as dotenv from 'dotenv';

dotenv.config();

// Voc√™ precisa de um token de https://hf.co/settings/tokens
// Certifique-se de selecionar 'read' como tipo de token
const HF_TOKEN = process.env.HF_TOKEN;

if (!HF_TOKEN) {
  throw new Error('HF_TOKEN n√£o encontrado nas vari√°veis de ambiente');
}

const client = new HfInference(HF_TOKEN);
```

### Teste B√°sico da API

```typescript
interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

async function testBasicChat(): Promise<void> {
  try {
    const response = await client.chatCompletion({
      model: "meta-llama/Llama-4-Scout-17B-16E-Instruct",
      messages: [
        { role: "user", content: "A capital da Fran√ßa √©" }
      ],
      max_tokens: 20,
      stream: false
    });

    console.log(response.choices[0].message.content);
    // Output: Paris.
  } catch (error) {
    console.error('Erro na chamada da API:', error);
  }
}
```

O m√©todo `chatCompletion` √© o m√©todo **RECOMENDADO** para usar a fim de garantir uma transi√ß√£o suave entre modelos.

## Agente Dummy

Nas se√ß√µes anteriores, vimos que o n√∫cleo de uma biblioteca de agentes √© anexar informa√ß√µes no system prompt.

Este system prompt √© um pouco mais complexo do que o que vimos anteriormente, mas j√° cont√©m:

- **Informa√ß√µes sobre as ferramentas**
- **Instru√ß√µes do ciclo** (Thought ‚Üí Action ‚Üí Observation)

```typescript
// System prompt mais complexo que j√° cont√©m a descri√ß√£o das fun√ß√µes anexadas
// Aqui supomos que a descri√ß√£o textual das ferramentas j√° foi anexada

const SYSTEM_PROMPT = `Answer the following questions as best you can. You have access to the following tools:

get_weather: Get the current weather in a given location

The way you use the tools is by specifying a json blob.
Specifically, this json should have an \`action\` key (with the name of the tool to use) and an \`action_input\` key (with the input to the tool going here).

The only values that should be in the "action" field are:
get_weather: Get the current weather in a given location, args: {"location": {"type": "string"}}
example use :

{
  "action": "get_weather",
  "action_input": {"location": "New York"}
}

ALWAYS use the following format:

Question: the input question you must answer
Thought: you should always think about one action to take. Only one action at a time in this format:
Action:

\$JSON_BLOB (inside markdown cell)

Observation: the result of the action. This Observation is unique, complete, and the source of truth.
(this Thought/Action/Observation can repeat N times, you should take several steps when needed. The \$JSON_BLOB must be formatted as markdown and only use a SINGLE action at a time.)

You must always end your output with the following format:

Thought: I now know the final answer
Final Answer: the final answer to the original input question

Now begin! Reminder to ALWAYS use the exact characters \`Final Answer:\` when you provide a definitive answer.`;
```

### Construindo as Mensagens

Precisamos anexar a instru√ß√£o do usu√°rio ap√≥s o system prompt. Isso acontece dentro do m√©todo chat:

```typescript
const messages: ChatMessage[] = [
  { role: "system", content: SYSTEM_PROMPT },
  { role: "user", content: "Qual √© o clima em Londres?" }
];

console.log(messages);
```

### Primeira Tentativa (Com Problema)

Vamos chamar o m√©todo chat:

```typescript
async function firstAttempt(): Promise<void> {
  const response = await client.chatCompletion({
    model: "meta-llama/Llama-4-Scout-17B-16E-Instruct",
    messages: messages,
    max_tokens: 200,
    stream: false
  });

  console.log(response.choices[0].message.content);
}

// Output problem√°tico:
// Thought: Para responder √† pergunta, preciso obter o clima atual em Londres.
// Action:
// ```
// {
//   "action": "get_weather",
//   "action_input": {"location": "London"}
// }
// ```
// Observation: O clima atual em Londres est√° parcialmente nublado com temperatura de 12¬∞C.
// Thought: Agora sei a resposta final.
// Final Answer: O clima atual em Londres est√° parcialmente nublado com temperatura de 12¬∞C.
```

**Voc√™ v√™ o problema?**

Neste ponto, o modelo est√° **alucinando**, porque est√° produzindo uma "Observation" fabricada ‚Äî uma resposta que gera por conta pr√≥pria em vez de ser o resultado de uma chamada de fun√ß√£o ou ferramenta real. Para prevenir isso, paramos de gerar logo antes de "Observation:". Isso nos permite executar manualmente a fun√ß√£o (ex: get_weather) e ent√£o inserir a sa√≠da real como a Observation.

### Solu√ß√£o: Usando o Par√¢metro "stop"

```typescript
interface ActionParsing {
  action: string;
  action_input: Record<string, any>;
}

async function fixedAttempt(): Promise<void> {
  // Para a gera√ß√£o antes da observa√ß√£o real!
  const response = await client.chatCompletion({
    model: "meta-llama/Llama-4-Scout-17B-16E-Instruct",
    messages: messages,
    max_tokens: 150,
    stream: false,
    // Paramos antes de qualquer fun√ß√£o ser executada
    stop: ["Observation:"]
  });

  console.log(response.choices[0].message.content);
}

// Output melhor:
// Thought: Para responder √† pergunta, preciso obter o clima atual em Londres.
// Action:
// ```
// {
//   "action": "get_weather",
//   "action_input": {"location": "London"}
// }
// ```
```

**Muito melhor!**

### Criando uma Fun√ß√£o Dummy para o Clima

Agora vamos criar uma fun√ß√£o dummy get_weather. Em uma situa√ß√£o real voc√™ chamaria uma API.

```typescript
// Fun√ß√£o dummy
function getWeather(location: string): string {
  return `o clima em ${location} est√° ensolarado com temperaturas baixas.\n`;
}

console.log(getWeather('Londres'));
// Output: 'o clima em Londres est√° ensolarado com temperaturas baixas.\n'
```

### Concatenando e Retomando a Gera√ß√£o

Vamos concatenar o system prompt, o prompt base, a conclus√£o at√© a execu√ß√£o da fun√ß√£o e o resultado da fun√ß√£o como uma Observation e retomar a gera√ß√£o.

```typescript
async function completeAgentFlow(): Promise<void> {
  // Primeira chamada - para antes da Observation
  const initialResponse = await client.chatCompletion({
    model: "meta-llama/Llama-4-Scout-17B-16E-Instruct",
    messages: messages,
    max_tokens: 150,
    stream: false,
    stop: ["Observation:"]
  });

  // Executar a fun√ß√£o real
  const weatherResult = getWeather('Londres');
  
  // Concatenar tudo
  const newMessages: ChatMessage[] = [
    { role: "system", content: SYSTEM_PROMPT },
    { role: "user", content: "Qual √© o clima em Londres?" },
    { 
      role: "assistant", 
      content: initialResponse.choices[0].message.content + "Observation: " + weatherResult
    }
  ];

  // Segunda chamada para completar a resposta
  const finalResponse = await client.chatCompletion({
    model: "meta-llama/Llama-4-Scout-17B-16E-Instruct",
    messages: newMessages,
    max_tokens: 200,
    stream: false
  });

  console.log("Resposta final:", finalResponse.choices[0].message.content);
}

// Output final:
// Thought: Agora sei a resposta final
// Final Answer: O clima em Londres est√° ensolarado com temperaturas baixas.
```

## Exemplo Completo Funcional

Aqui est√° um exemplo completo que voc√™ pode executar:

```typescript
import { HfInference } from '@huggingface/inference';
import * as dotenv from 'dotenv';

dotenv.config();

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

class DummyAgent {
  private client: HfInference;
  private systemPrompt: string;

  constructor(hfToken: string) {
    this.client = new HfInference(hfToken);
    this.systemPrompt = `Answer the following questions as best you can. You have access to the following tools:

get_weather: Get the current weather in a given location

The way you use the tools is by specifying a json blob.
Specifically, this json should have an \`action\` key (with the name of the tool to use) and an \`action_input\` key (with the input to the tool going here).

The only values that should be in the "action" field are:
get_weather: Get the current weather in a given location, args: {"location": {"type": "string"}}

ALWAYS use the following format:

Question: the input question you must answer
Thought: you should always think about one action to take. Only one action at a time.
Action:
\`\`\`json
$JSON_BLOB
\`\`\`
Observation: the result of the action. This Observation is unique, complete, and the source of truth.

You must always end your output with:
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Now begin! Reminder to ALWAYS use the exact characters \`Final Answer:\` when you provide a definitive answer.`;
  }

  // Fun√ß√£o dummy para clima
  private getWeather(location: string): string {
    return `o clima em ${location} est√° ensolarado com temperaturas baixas.\n`;
  }

  // Executa uma query completa
  async query(userInput: string): Promise<string> {
    const messages: ChatMessage[] = [
      { role: "system", content: this.systemPrompt },
      { role: "user", content: userInput }
    ];

    // Primeira chamada - para antes da Observation
    const initialResponse = await this.client.chatCompletion({
      model: "meta-llama/Llama-4-Scout-17B-16E-Instruct",
      messages: messages,
      max_tokens: 150,
      stream: false,
      stop: ["Observation:"]
    });

    console.log("Resposta inicial:", initialResponse.choices[0].message.content);

    // Executar a fun√ß√£o (aqui seria onde parseamos o JSON e chamamos a fun√ß√£o real)
    const weatherResult = this.getWeather('Londres');
    
    // Concatenar com a observation real
    const newMessages: ChatMessage[] = [
      { role: "system", content: this.systemPrompt },
      { role: "user", content: userInput },
      { 
        role: "assistant", 
        content: initialResponse.choices[0].message.content + "Observation: " + weatherResult
      }
    ];

    // Segunda chamada para completar
    const finalResponse = await this.client.chatCompletion({
      model: "meta-llama/Llama-4-Scout-17B-16E-Instruct",
      messages: newMessages,
      max_tokens: 200,
      stream: false
    });

    return finalResponse.choices[0].message.content;
  }
}

// Uso
async function main(): Promise<void> {
  const hfToken = process.env.HF_TOKEN;
  if (!hfToken) {
    throw new Error('HF_TOKEN n√£o encontrado');
  }

  const agent = new DummyAgent(hfToken);
  
  try {
    const result = await agent.query("Qual √© o clima em Londres?");
    console.log("Resultado final:", result);
  } catch (error) {
    console.error('Erro:', error);
  }
}

main();
```

## Conclus√£o

Aprendemos como podemos criar Agentes do zero usando c√≥digo TypeScript, e vimos como esse processo pode ser tedioso. Felizmente, muitas bibliotecas de Agentes simplificam esse trabalho lidando com muito do trabalho pesado para voc√™.

**Agora estamos prontos para criar nosso primeiro Agente real usando a biblioteca smolagents!**

---

## üì¶ Pacotes Necess√°rios

Para executar este exemplo, instale os seguintes pacotes:

```bash
# Depend√™ncias principais
npm install @huggingface/inference dotenv

# Depend√™ncias de desenvolvimento  
npm install -D @types/node typescript ts-node

# Configurar TypeScript
npx tsc --init
```

### Arquivo .env
Crie um arquivo `.env` na raiz do projeto:
```
HF_TOKEN=seu_token_aqui
```

### Execu√ß√£o
```bash
npx ts-node agent.ts
```